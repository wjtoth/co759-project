\section{Introduction}
\label{S:1}
\paragraph{}
Neural networks were a classic development in artificial intelligence research, beginning as early as the 1950s. Neural networks are computing systems made up of artificial neurons behaving in analogy with our present understanding of the function of the neurons in the brain. However, given the complexity of the human brain, modeling brain function in a neural network was a very ambitious thought at the time of the inception of neural networks. As a result, early neural networks were extremely basic and came nowhere near simulating real biological neuron function \cite{mcculloch1990logical}. More specifically, early neural networks could only simulate a very limited number of neurons at once, which meant that they could not recognize complex patterns (e.g. images, languages and rules of a game). \\
 
Since that time there has been tremendous progress in the efficacy of neural networks. In the past decade Hinton, Bengio, and a host of their contemporaries have made fundamental breakthroughs\cite{lecun1989backpropagation}\cite{hinton2007learning}\cite{bengio2013estimating}\cite{bengio2014auto}\cite{lee2015difference}\cite{hubara2016quantized} in what we know today as \textit{deep neural networks}, ultimately leading to networks capable of human-level, or near human-level, performance on a variety of tasks. These include but are not limited to image classification\cite{cirecsan2012multi}, speech recognition\cite{deng2014deep}\cite{schmidhuber2015deep}, machine translation\cite{hochreiter1997long}\cite{johnson2016google}, and playing video and board games\cite{justesen2017deep}.\\

Deep neural networks are made up of many layers of artificial neurons that are ``hidden" between the input and output layers. We call such layers ``hidden layers". Intuitively each hidden layer helps in learning different representations of input data, thus allowing the network to learn complex patterns built up from more basic concepts. As deep neural networks get deeper (more hidden layers) and wider (more neurons per layer), the computational demands of the network for both training and classification grow rapidly. The computing architecture required for such massive networks is often prohibitively expensive. Network quantization offers a solution to improve the scaling of deep neural networks\cite{hubara2016quantized}. \\

Quantized neural networks reduce computational complexity and enable low-precision inference by running with low-precision activations and/or weights and training with full complexity \cite{hubara2016quantized}. One way to reduce the complexity is to use a hard-threshold activation function on each unit. An example of such a function is the sign function which outputs $1$ if its input is non-negative, and $-1$ otherwise. The output of the sign function can be represented with a single bit. While most deep neural networks are trained by minimizing an error function using backpropagation via gradient descent, these techniques cannot be implemented when learning networks with hard-threshold activations because the derivative of such functions are zero everywhere and, non-differentiable at the origin. As a result training these networks requires the development of novel ideas. \\ 

We are interested in using a method called \emph{target propagation} to learn deep neural networks with hard-threshold activations. Target propagation involves computing target values for each unit at each layer, instead of computing gradients\cite{lee2015difference}. Each feedforward unit's activation value is explicitly associated with a desired target value, as opposed to simply the outputs at the final (non-hidden) layer. Our algorithm generates these hidden targets and uses them to train the network. The weights at each layer are updated locally in order to make the activation values closer to the targets. Similar to gradient descent, target propagation is propagated backwards; by treating each layer (starting from the output layer) as an individual perceptron, we work backwards through the network to update the weights working with each layer separately\cite{lee2015difference}. In general, we use the $(i+1)^\text{st}$ layer's targets to set the $i^\text{th}$ layer's targets and then update the $(i+1)^\text{st}$ layer's weights. \\

In Section $3$ we explore the question of how to set these target values at each layer. As observed in \cite{friesen2017deep}, setting the target values breaks down into a discrete optimization problem minimizing the training loss. Building off of Friesen and Domingos work, we are interested in developing efficient techniques for learning hard-threshold deep neural networks. We expand on their earlier work by proposing a host of target generation heuristics, implementing some based on combinatorial search, and initialize a line of experimental investigation into which target generation heuristics are most effective for target propagation. We develop a precise specification for a mini-batch target propagation method and test different target generation heuristics in this framework. The network we experiment with is a 4-layer convolutional deep neural network. For our hard-threshold activation function, we use the standard sign function.\\

In Section $4$ we provide experimental evidence lending support to the hypothesis that hard-threshold networks are less susceptible to adversarial attacks. This admits another motivation for studying such networks beyond just network quantization. The working hypothesis is that the discontinuities in the activation units make it more difficult for an adversary to construct mis-classified examples than the highly continuous, even linear, units used in standard practice today. While our results are positive, we stress that more testing is necessary to fully confirm this hypothesis. 


\section{Related work}
\label{S:2}

\subsection{Backpropagation for quantized networks}

Using the conventional back-propagation algorithm via gradient descent to train a neural network, we compute the gradient of the loss with respect to the output of each layer : $$\partial h_{i-1}=\frac{\partial L}{\partial h_{i-1}}=\frac{\partial L}{\partial h_i}\frac{\partial h_i}{\partial h_{i-1}}=\partial h_i \frac{\partial h_i}{\partial h_{i-1}}$$ where $h_i$ is the output of layer $i$ \cite{lee2015difference}. This is propagated recursively from the output layer to the input layer using the chain rule. In our case $h_i$ would involve a non-linearity at layer $i$. This non-linearity does not allow us to take partial derivatives, hence ruling out the use of gradient descent. The most common method for learning deep hard-threshold neural networks using back-propagation is to use a gradient estimator, such as the straight-through estimator (STE) . The STE was first proposed by Geoffrey Hinton in 2012 \cite{hinton2012neural}, and later analyzed by Bengio in 2013 \cite{bengio2013estimating}. The idea is to replace the derivative of each hard-threshold unit with the identity function.\\

\subsection{Target propagation}
The idea of target propagation was first proposed in \cite{lecun1989backpropagation}, although the term was not yet coined at that time. Since then, many different methods of target propagation have been proposed \cite{bengio1995input}\cite{courbariaux2015binaryconnect}. In particular, Bengio\cite{bengio2014auto} proposed a method of target propagation called ``the vanilla target propagation" to avoid the chain of derivatives through the network by considering an ``approximate inverse" denoted by $g_i$ such that: $$f_i(g_i(\hat{h}_i)) \approx \hat{h}_i$$ where $f_i$ is the output function for layer $i$, $\hat{h}_i$ is the target associated with $h_i$, and $\hat{h}_{i-1}=g_i(\hat{h}_i)$. We call $g_i$ the \emph{approximate inverse} of $f_i$, since $g_i$ need not invert all of $f_i$, only the neighbourhood of targets. Later \cite{hubara2016quantized} improved this method, claiming that ``the imperfection of the inverse yields severe optimization problems". This led to their improved method called ``difference target propagation", which is a linearly corrected formula of Bengio: $$\hat{h}_{i-1}-h_{i-1}=g(\hat{h}_i)-g_i(h_i)$$ In order to fix the problem, they require the following stability condition: $h_i = \hat{h}_i \rightarrow h_{i-1}=\hat{h}_{i-1}$. They claim that if this is not the case, then even though optimization will be terminated in the upper layers, the weights in the lower lays would continue to update. They showed that the output of each layer came closer to its target than the method of \cite{bengio2014auto}. They also showed that this method of target propagation performs comparable to back-propagation methods. \\

A method for training Quantized Neural Networks (QNNs), called Binarized Neural Networks (BNNs) was introduced by \cite{hubara2016quantized}. The idea is to use only 1-bit per weight and activation, i.e. both weights and activations are constrained to either +1 or -1. They show that a binarized convolutional neural network (CNN) can reduce time complexity by 60\%. In our model, we can think of our activations to be binarized.\\ 

Most recently, \cite{friesen2017deep} show that learning a deep hard-threshold network reduces to finding a feasible setting of its targets and then optimizing its weights in a layer-local fashion. In other words, learning such a network can viewed as a mixed convex-combinatorial optimization problem.

\subsection{Loss Functions}

When training deep neural networks with hard-threshold units, there are a few commonly-used loss functions throughout the existing literature. In \cite{lee2015difference}, they use a mean squared error (MSE) to calculate loss at each later. In their actual implementation of ``difference target propagation", they use a modified version of MSE by adding a ``regularization" term to obtain a contraction mapping. In \cite{hubara2016quantized}, they use two different loss functions. In their convolutional network architecture using CIFAR-10, they use a mean squared hinge loss, and in their AlexNet architecture, they use a negative log likelihood. Another type of loss function, mentioned in \cite{friesen2017deep} is the hinge loss function. This type of loss function is a conventional per-layer loss function to use for generating feasible targets and weights. They show that this type of loss function tended to stall and produced more errors over time. It has been shown in \cite{wu2007robust} that hinge loss is sensitive to noisy data and outliers which can cause flaws in the process of learning. Oner remedy is the ``saturated" hinge loss function which is less sensitive to noise and outliers. The only problem with this type of function is that if the input value ever moved out of the range $[-1,+1]$, then the derivative would be zero. As a result, Friesen and Domingos propose to use a ``soft" hinge loss, which is a modification of hyperbolic tangent. This function has a slope of $1$ at the threshold and has a larger input region with non-zero derivative \cite{friesen2017deep}. It was with the soft hinge loss function that they obtained their results.   


\subsection{Adversarial robustness in neural networks}
Recently, Szegedy et al. \cite{szegedy2014} discovered that deep neural classifiers are susceptible to misclassifying an otherwise correctly classified image by applying an imperceptible perturbation designed to maximize the classifier's prediction error. Given that their technique, which involves using box-constrained L-BFGS to search for optimally-fooling perturbations, requires knowledge of the network's internals, it is a \textit{white-box} attack method. Many works since then have similarly resorted to using white-box attack methods to generate adversarial examples, including those that utilize loss gradients (e.g. \cite{goodfellow2014, dezfooli2016, papernot2016a, rauber2017}). Although these cannot be used by a conventional attacker who has not already broken other standard security measures, and may be naturally less effective against hard-threshold neural networks, \textit{black-box} methods for generating adversarial examples have also been demonstrated and proven effective against a wide variety of models and training methods (e.g. \cite{papernot2017, liu2016}).

Despite the fact that they are not generic in the space of input data, the existence of these \textit{adversarial examples} could lead to significant safety and security risks as DNNs gain increasing use in real-world settings \cite{yuan2018}. Consequently, there has been a great deal effort spent working towards the goal of training networks that are robust to adversarial examples. The most commonly used technique for defending against adversarial examples is to include both clean and adversarial inputs in the training data (e.g. \cite{szegedy2014, goodfellow2014, tramer2018, madry2018}). Last year, Madry et al. used a variant on this defense to achieve over 89\% and 46\% accuracy on the MNIST and CIFAR10 image classification datasets, respectively, against the strongest known white-box and black-box adversaries. This was improved further by recent work that involves modifying the loss function \cite{kannan2018} or applying an input discretization method \cite{buckman2018} to encourage adversarial resistance. 

Goodfellow et al. \cite{goodfellow2014} were the first to provide experimental evidence for the hypothesis that the highly \textit{linear} nature of common neural networks is largely responsible for their vulnerability to adversarial examples. Accordingly, they demonstrated that one highly nonlinear model, a radial basis function (RBF) network, is naturally  resilient to adversarial examples. Along these lines, we aim to investigate the robustness of hard-threshold networks to adversarial examples; given recent results improving the training of such networks, hard-threshold functions may prove to be a more viable substitute for common activation functions (e.g. ReLU, sigmoid, etc.) than radial basis functions, which are known to perform poorly on common tasks such as image classification. 