\section{Resistance to Adversarial Examples}

It has been speculated that deep neural networks with hard-threshold activations may be less vulnerable to adversarial examples \cite{friesen2017deep}. In particular, the discreteness of the hard-threshold activations may help mask the effects of small perturbations to an input. This is further supported by past work implicating the highly linear nature of most neural networks as the primary cause of high adversarial example vulnerability \cite{goodfellow2014}. In addition, Goodfellow et al. also directly demonstrated that RBF networks, another highly nonlinear model, are relatively robust to adversarial examples, achieving significantly lower classification error on adversarial examples generated using a white-box attack than maxout networks and logistic regression models. Given their superior performance on image classification tasks, approaching the level of standard ReLU networks in some cases \cite{friesen2017deep}, we are interested in determining whether hard-threshold networks are similarly robust to adversarial examples. 

\subsection{White-Box Attacks} %White Dog Attacks

In our experiments, we apply the following standard gradient-based white-box methods for generating adversarial examples: 

\begin{itemize}
\item Fast gradient sign method (FGSM) \cite{goodfellow2014}: Given a correctly classified example $x$ with target label $l$, a perturbation constant $\epsilon > 0$, and loss function $J$, the adversarial example $x'$ is calculated by applying an optimal max-norm constrained fooling perturbation in the direction of the loss gradient:
\begin{align*}
x' = x + \epsilon\textrm{sign}(\nabla_{x}J(x, l)).
\end{align*}
\item Iterative fast gradient sign method (I-FGSM) \cite{dong2017}: This method extends the previous one by iteratively applying FGSM and incorporating momentum into the generation process, resulting in a more effective attack.
\item DeepFool (DF) \cite{dezfooli2016}: A simple yet powerful iterative method that treats the network as a linear model and thereby computes an (approximately) optimal perturbation at each step. 
\item Jacobian-based Saliency Map
Attack (JSMA) \cite{papernot2016b}:  This method uses logit gradients to compute a \textit{saliency map} indicating the impact of each pixel in the input on the classification. It then picks the most important pixel and modifies it to maximize the likelihood of a given target class, and repeats this process iteratively to produce an adversarial example.
\end{itemize}


\subsection{Black-Box Attacks}

\begin{itemize}
\item Single pixel attack \cite{su2017}: This novel method uses differential evolution to generate a fooling perturbation by modifying a single pixel of the original image. 
\item Greedy local search method \cite{narodytska2016}: Similar to the above method, greedy local search on a randomly-generated perturbation is used to find an adversarial example generated by altering a small set of pixels in an image. 
\item Boundary Attack \cite{brendel2018}: A powerful decision-based attack that starts with a large perturbation and iteratively adds new random perturbations to a given image if they push the image towards fooling the classifier. This method achieves competitive performance with many gradient-based white-box attacks. 
\end{itemize}


\subsection{Experiments and Results}

To evaluate the adversarial robustness of hard-threshold networks, we trained a standard 4-layer convolutional neural network with ReLU and sign function activations on CIFAR-10, a standard image classification dataset involving 60,000 real-world images, each labeled with one of 10 classes. The ReLU network was trained using stochastic gradient descent via backpropagation, while the hard-threshold network was trained used the gradient-based target propagation method described in Section 3.3. Due to time and compute constraints, each network was trained for only 100 epochs with a batch size of 64 using the Adam optimizer \cite{kingma2015} with weight decay and a learning rate of 0.00025. After training, we evaluated each of the networks on each of the seven adversarial attacks using implementations provided by the FoolBox adversarial example library \cite{rauber2017}. The error rates of each network on a clean held-out dataset of 6000 images and on adversarial datasets of 1000 images constructed with each of the seven attacks are displayed in Table 1. 

\renewcommand{\arraystretch}{1.2}
\begin{table}[H]
\centering
\caption{Error rates (in \%) on adversarial examples.}
\begin{tabular}{@{}lrrrrrrrr@{}}
 & & \multicolumn{4}{c}{Gradient-based} & \multicolumn{3}{c}{Score/Decision-based}  \\ 
\cmidrule{3-6} \cmidrule{7-9}
Model & Clean & FGSM & I-FGSM & DF & JSMA & 1-Pixel & Search & Boundary  \\ \hline
CNN4-ReLU & \textbf{17.42} & 89.90 & 89.86 & 90.8 & 90.8 & 91.00 & 62.89 & 90.5  \\ \hline
CNN4-Step & 33.6 & \textbf{60.56} & \textbf{60.56} & \textbf{7.58} & \textbf{60.86} & \textbf{46.57} & \textbf{58.46} & \textbf{62.42}  \\
\bottomrule
\end{tabular}
\end{table}

Mirroring the results of \cite{friesen2017deep}, the ReLU network performs significantly better on clean examples than the hard-threshold network, achieving almost a factor of two lower error rate. Yet the hard-threshold network achieves substantially lower errors rates than the ReLU network under every attack; in particular, the former typically correctly classifies over 3 times as many adversarial examples as the latter. In the case of DeepFool, the difference in performance is even greater---in fact, the hard-threshold network achieves a lower error rate on the adversarial dataset than on the clean dataset, while the ReLU is reduced to the accuracy rate of a random classifier (this is possible because adversarial examples are only generated using correctly classified examples). Note that this may be a result of the fact that every gradient-based attack must rely on approximate gradients in the case of the hard-threshold network; in particular, gradients at each hidden layer across the activation function are replaced with loss gradients computed at that layer via target propagation (see Section 3.3). Although this may afford hard-threshold networks a natural robustness to gradient-based attacks, our results do not suggest such a conclusion; in fact, other than DeepFool, the hard-threshold network generally has higher error rates on gradient-based white-box attacks than decision-based black-box attacks. 

Note that the adversarial misclassification rate of the ReLU network is generally limited to around 90\% likely because we limited the number of steps in all iterative attacks in order to reduce compute time. Also, note that these results are tentative and subject to significant further experimentation in order to definitively implicate the hard-threshold activation function as the primary contributing factor to the adversarial robustness exhibited by our hard-threshold network.

