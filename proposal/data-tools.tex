\section{Training Data and Programming Tools}\label{tools}
\paragraph{}
The paper applies the proposed training method to four neural networks:

\begin{itemize}
\item A simple 4-layer convolution network,
\item the 8-layer convolution network of Zhou \cite{zhou2016dorefa},
\item AlexNet \cite{alexnet},
\item ResNet-18 \cite{resnet18}.
\end{itemize}

Applied to two datasets:

\begin{itemize}
\item CIFAR-10, which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. (\url{https://www.cs.toronto.edu/~kriz/cifar.html}).
\item ImageNet (ILSVRC 2012), which consists of 1.2 million photographs, collected from flickr and other search engines, hand labeled with the presence or absence of 1000 object categories (\url{http://image-net.org/challenges/LSVRC/2012/}). The ILSVRC is the ImageNet Large Scale Visual Recognition Challenge, which takes place annually.
\end{itemize}

The first two networks were trained on CIFAR-10, while AlexNet and Resnet-18 were trained on ImageNet. 

We obtained the source code containing implementations of all models, training methods, and data processing used for the paper from the authors by e-mail. All code was written in Python using PyTorch as the neural network framework, and we plan to use the same tools.

Our experiments will then encompass six different models, the four that the original paper compared to, the original hard-threshold activation model, and an improved model that we wish to attain.