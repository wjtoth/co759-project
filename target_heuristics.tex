\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{mathdots}
\usepackage{titlesec}
\usepackage[backend=bibtex]{biblatex}
\usepackage{hyperref, mathrsfs}
\usepackage[]{footmisc}
\usepackage{graphicx, color}
\usepackage{verbatim, textcomp}
\usepackage{algorithm, algorithmic}
\titleformat{\section}{\normalsize\scshape\center}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\scshape\center}{\thesubsection}{1em}{}
\makeatletter
\renewcommand\@makefntext[1]{%
    \parindent 1em%
    \@thefnmark.~#1}
\makeatother

\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother

\begin{document}
\title{\uppercase{\textbf{\normalsize Improving and Evaluating the Use of Mixed Convex-Combinatorial Optimization for Deep Learning}}}
%%% Improving and Evaluating the Training of Hard-Threshold Deep Neural Networks
%%% or, On 
\author{\small{\textsc{}}}
\date{\small{\textsc{\today}}}
\maketitle

\allowdisplaybreaks

\section{Gradient-based Methods and Adversarial Examples}

For a $l$-layer deep neural network with hard-threshold units, let $h_{dj} \in \{-1, 1\}$ be the $j$th output activation in the $d$th hidden layer, $z_{dj}$ be the pre-activation output, and $t_{dj}$ be the target used during training via target propagation. Furthermore, write $T_{d} = \{t_{d1}, \ldots, t_{dk}\}$ and $Z_{d} = \{z_{d1}, \ldots, z_{dk}\}$, where $k$ is the number of units in the $d$th hidden layer. 

The ``fast gradient sign method" (FGSM) for generating adversarial examples computes a max-norm constrained perturbation $\mathbf{\eta}$ for ``optimally" fooling the network given by
$$\mathbf{\eta} = \epsilon \cdot \textrm{sign}(\nabla_{\mathbf{x}}L(\theta, \mathbf{x}, y)),$$ 
where $\mathbf{x}$ is an vector input to the network, $y$ is a scalar output target associated to $\mathbf{x}$, $L$ is the loss function used to train the network, and $\epsilon$ is a small perturbation constant; the adversarial input is then $\tilde{\mathbf{x}} = \mathbf{x} + \mathbf{\eta}$. If we instead use $-\mathbf{\eta}$ to perturb $\mathbf{x}$, then we obtain an input $\tilde{\mathbf{x}}$ that ``optimally" achieves the desired target $y$ when processed by the network. But this is essentially exactly what we need to set the target $t_{dj}$ for target propagation---in particular, for the loss function $L_{d+1}$ dependent on $Z_{d}$ and $T_{d}$ associated to layer $d$ of our network, letting
$$t_{dj} = -\textrm{sign}\left({\partial \over \partial h_{dj}}L_{d+1}(Z_{d+1}, T_{d+1})\right)$$
gives an effective setting of the targets (for training the network via target propagation) using a method analogous to applying FGSM to create an adversarial example for the perceptron at layer $d$. Note that this is exactly the target heuristic used in the Friesen et al. paper, giving independent justification for the method. 

This also immediately invites potential improvements to the heuristic. If we apply a gradient estimator for the hard-threshold units (like the straight-through estimator), we could instead replace $L_{d+1}$ with the output loss $L$ and use backpropagation in combination with target propagation to train the network. Furthermore, we could also draw on additional methods from the adversarial examples and nonconvex optimization literature in order to improve the target setting heuristic; for example, letting a superscript $t$ denote the value of a variable at training time step $t$, we can apply momentum to target setting in (something like) the following way:
$$t_{dj}^{t+1} = \textrm{sign}\left(\mu t_{dj}^{t} - {{\partial \over \partial h_{dj}}L_{d+1}(Z_{d+1}, T_{d+1}) \over \Vert \nabla_{h_{d}}L_{d+1}(Z_{d+1}, T_{d+1}) \Vert}\right).$$



\section{Combinatorial Optimization}

Using a combinatorial optimization method (e.g. beam search, or a genetic algorithm) to set the targets for training a neural network via target propagation has the potential to make the continuous optimization problem more tractable by restricting it to a single one-layer perceptron and the combinatorial optimization problem more tractable by vastly reducing the search space from around $10^{5}$--$10^{10}$ parameters to only $10^{2}$--$10^{5}$ targets---although the targets vary per training example, and there are typically millions of examples processed during training, so the problem is not necessarily more tractable. 

The target propagation training algorithm looks something like the following (see Algorithms 1 and 2):

\begin{algorithm}
\caption{Mixed Convex-Combinatorial Target Propagation}
\begin{algorithmic}  
\FOR{each batch of training examples}
\STATE{Run a forward pass through the network to obtain the activations (hidden state) for each layer}
\STATE{Choose a population of candidate targets for each layer}
\FOR{each layer, starting at the last layer}
\FOR{each candidate target in the population}
\STATE{Run a step of gradient descent on the perceptron associated to this layer based on $L(h, t)$, where $L$ is the loss function for the perceptron, $h$ is the hidden state, and $t$ is the candidate target}
\STATE{Given the hidden state of the previous layer, run a forward pass through this layer and the next layer and calculate $L_{\textrm{next}}$, the next layer's loss (bootstrapping).}
\ENDFOR
\STATE{Choose targets and weights for this layer based on the losses.}
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm} 

\begin{algorithm}
\caption{Efficient Mixed Convex-Combinatorial Target Propagation}
\begin{algorithmic}  
\FOR{each batch of training examples}
\STATE{Choose a population of candidate targets for each layer}
\FOR{each layer, starting at the last layer}
\IF{at first layer}
\STATE{Run a step of gradient descent on the perceptron associated to this layer based on $L(x, t_{\textrm{curr}})$, where $L$ is the loss function for the perceptron, $x$ is the input provided by the batch, and $t_{\textrm{curr}}$ is the current layer's target}
\ELSE
\FOR{each candidate target $t$ for the previous layer in the population}
\STATE{Given the target $t$ as input, run a forward pass through this layer and calculate $L(h, t)$, where $h$ is the hidden state and $t_{\textrm{curr}}$ is the target associated to this layer}
\ENDFOR
\STATE{Choose the target vector $t_{\textrm{prev}}$ for the previous layer based on the losses}
\STATE{Run a step of gradient descent on the perceptron associated to this layer based on $L(h, t_{\textrm{curr}})$, where $h$ is the hidden state associated to $t_{\textrm{prev}}$}
\ENDIF
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm} 

We may consider augmenting this method in various ways. For example, it may be beneficial to choose some candidate targets from past time steps with training examples of the same output target, a kind of target ``momentum". Note that this example augmentation would primarily make sense for targets in layers close to the output, since earlier layers tend to process detailed features that could deviate too greatly between inputs with the same output target (although this is only a problem to some extent). Furthermore, we might also choose some candidate targets derived from a gradient-based method as described above, allowing for a mixed continuous-combinatorial target setting approach that achieves the benefits of both while adding only a small overhead from computing the gradients.


\section{Performance and Parallelization}

Note that Algorithm 2, our efficient algorithm for mixed convex-combinatorial target propagation, has complexity $O(nlk^{2}p)$ for a vanilla feedforward neural network with $l$ layers and $k$ hidden units per layer, a population size of $p$ targets per layer per training step, and $n$ training examples, where we assume the batch size is 1.\footnote{This is a safe assumption since parallelizing training across the batch can be done efficiently.} On the other hand, standard continuous optimization via stochastic gradient descent and backpropagation has complexity $O(nlk^{2})$, the only significant difference in complexity arising from the use of a population of targets and the need for doing a forward pass through the network for each candidate target. Note though that each step in the second nested for-loop in Algorithm 2 only requires targets computed in the previous step, and each step in the inner for-loop that involves evaluating a candidate target is independent from every other step. Consequently, we can parallelize computation across both the target population and across batches---as one processor is running a step of the algorithm 
at layer $d$ for batch $1$, another process can be running a step of the algorithm at layer $d+1$ for batch $2$, etc. Assuming equivalent computation time across batches and layers, we can process up to $l$ batches in parallel, one for each layer, potentially mitigating the overhead involved in parallelizing computations for individual layers. 

Specifically, for implementing parallelization, what probably makes sense is to send to each worker/processor a set of around $l$ jobs, each of which involves a distinct batch, a distinct layer, and a small set of candidate targets for evaluation (perhaps 1, or up to maybe 5 or 10). This results in on the order of 100 forward passes for each worker and each set of jobs---given that it's only a forward pass through a single layer perceptron, this could be run fully in parallel on a single GPU, or on a single CPU. If this distributed setup does not produce a sufficiently high computation-communication ratio, we may be able to increase performance by sending more candidate targets or consecutive layers (and doing a few sets of computations per worker per job in serial). In addition to collating multiple candidate targets into a batch for evaluation, we can also place the forward graph of multiple candidate targets in series for memory-efficient GPU/CPU computation since, unlike for batch-wise computations, layer-wise computations can drop the results of most previous computations in order to run a very large graph (network) on a single processor. 




\end{document} 