\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{mathdots}
\usepackage{titlesec}
\usepackage[backend=bibtex]{biblatex}
\usepackage{hyperref, mathrsfs}
\usepackage[]{footmisc}
\usepackage{graphicx, color}
\usepackage{verbatim, textcomp}
\usepackage{algorithm, algorithmic}
\titleformat{\section}{\normalsize\scshape\center}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\scshape\center}{\thesubsection}{1em}{}
\makeatletter
\renewcommand\@makefntext[1]{%
    \parindent 1em%
    \@thefnmark.~#1}
\makeatother

\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother

\begin{document}
\title{\uppercase{\textbf{\normalsize Improving and Evaluating the Training of Hard-Threshold Deep Neural Networks: \\
Target Heuristics}}}
%%% or, On the Use of Mixed Convex-Combinatorial Optimization for Deep Learning
\author{\small{\textsc{}}}
\date{\small{\textsc{\today}}}
\maketitle

\allowdisplaybreaks

\section{Gradient-based Methods and Adversarial Examples}

For a $d$-layer deep neural network with hard-threshold units, let $h_{dj} \in \{-1, 1\}$ be the $j$th output activation in the $d$th hidden layer, $z_{dj}$ be the pre-activation output, and $t_{dj}$ be the target used during training via target propagation. Furthermore, write $T_{d} = \{t_{d1}, \ldots, t_{dk}\}$ and $Z_{d} = \{z_{d1}, \ldots, z_{dk}\}$, where $k$ is the number of units in the $d$th hidden layer. 

The ``fast gradient sign method" (FGSM) for generating adversarial examples computes a max-norm constrained perturbation $\mathbf{\eta}$ for ``optimally" fooling the network given by
$$\mathbf{\eta} = \epsilon \cdot \textrm{sign}(\nabla_{\mathbf{x}}L(\theta, \mathbf{x}, y)),$$ 
where $\mathbf{x}$ is an vector input to the network, $y$ is a scalar output target associated to $\mathbf{x}$, $L$ is the loss function used to train the network, and $\epsilon$ is a small perturbation constant; the adversarial input is then $\tilde{\mathbf{x}} = \mathbf{x} + \mathbf{\eta}$. If we instead use $-\mathbf{\eta}$ to perturb $\mathbf{x}$, then we obtain an input $\tilde{\mathbf{x}}$ that ``optimally" achieves the desired target $y$ when processed by the network. But this is essentially exactly what we need to set the target $t_{dj}$ for target propagation---in particular, for the loss function $L_{d+1}$ dependent on $Z_{d}$ and $T_{d}$ associated to layer $d$ of our network, letting
$$t_{dj} = -\textrm{sign}\left({\partial \over \partial h_{dj}}L_{d+1}(Z_{d+1}, T_{d+1})\right)$$
gives an effective setting of the targets (for training the network via target propagation) using a method analogous to applying FGSM to create an adversarial example for the perceptron at layer $d$. Note that this is exactly the target heuristic used in the Friesen et al. paper, giving independent justification for the method. 

This also immediately invites potential improvements to the heuristic. If we apply a gradient estimator for the hard-threshold units (like the straight-through estimator), we could instead replace $L_{d+1}$ with the output loss $L$ and use backpropagation in combination with target propagation to train the network. Furthermore, we could also draw on additional methods from the adversarial examples and nonconvex optimization literature in order to improve the target setting heuristic; for example, letting a superscript $t$ denote the value of a variable at training time step $t$, we can apply momentum to target setting in (something like) the following way:
$$t_{dj}^{t+1} = \textrm{sign}\left(\mu t_{dj}^{t} - {{\partial \over \partial h_{dj}}L_{d+1}(Z_{d+1}, T_{d+1}) \over \Vert \nabla_{h_{d}}L_{d+1}(Z_{d+1}, T_{d+1}) \Vert}\right).$$



\section{Combinatorial Optimization}

Using a combinatorial optimization method (e.g. beam search, or a genetic algorithm) to set the targets for training a neural network via target propagation has the potential to make the continuous optimization problem more tractable by restricting it to a single one-layer perceptron and the combinatorial optimization problem more tractable by vastly reducing the search space from around $10^{5}$--$10^{10}$ parameters to only $10^{2}$--$10^{5}$ targets---although the targets vary per training example, and there are typically millions of examples processed during training, so the problem is not necessarily more tractable. 

The target propagation training algorithm looks something like the following:

\begin{algorithm}
\caption{}
\begin{algorithmic}  
\FOR{each batch of training examples}
\STATE{Run a forward pass through the network to obtain the activations (hidden state) for each layer}
\STATE{Choose a population of candidate targets for each layer}
\FOR{each layer, starting at the last layer}
\FOR{each candidate target in the population}
\STATE{Run a step of gradient descent on the perceptron associated to this layer based on $L(h, t)$, where $L$ is the loss function for the perceptron, $h$ is the hidden state, and $t$ is the candidate target}
\STATE{Given the hidden state of the previous layer, run a forward pass through this layer and the next layer and calculate $L_{\textrm{next}}$, the next layer's loss (bootstrapping).}
\ENDFOR
\STATE{Choose targets and weights for this layer based on the losses.}
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm} 

We may consider augmenting this method in various ways. For example, it may be beneficial to choose some candidate targets from past time steps with training examples of the same output target, a kind of target ``momentum". Note that this example augmentation would primarily make sense for targets in layers close to the output, since earlier layers tend to process detailed features that could deviate too greatly between inputs with the same output target (although this is only a problem to some extent). Furthermore, we might also choose some candidate targets derived from a gradient-based method as described above, allowing for a mixed continuous-combinatorial target setting approach that achieves the benefits of both while adding only a small overhead from computing the gradients.


\end{document} 